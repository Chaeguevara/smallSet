{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('../datasets/train.csv')\n",
    "test = pd.read_csv('../datasets/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Before we start exploring embeddings lets write a couple of helper functions to run Logistic Regression and calculate evaluation metrics\n",
    "\n",
    "Since we want to optimize our model for F1-Scores, for all models we'll first predict the probability of the positive class. We'll then use these probabilities to get the Precision-Recall curve and from here we can select a threshold value that has the highest F1-score. To predict the labels we can simply use this threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "sns.set_palette(\"muted\")\n",
    "    \n",
    "\n",
    "def calc_f1(p_and_r):\n",
    "    p, r = p_and_r\n",
    "    return (2*p*r)/(p+r)\n",
    "\n",
    "\n",
    "# Print the F1, Precision, Recall, ROC-AUC, and Accuracy Metrics \n",
    "# Since we are optimizing for F1 score - we will first calculate precision and recall and \n",
    "# then find the probability threshold value that gives us the best F1 score\n",
    "\n",
    "def print_model_metrics(y_test, y_test_prob,y_pred,label_list,name):\n",
    "    print(\"*\"*5)\n",
    "    print(name)\n",
    "    print(\"roc\")\n",
    "    print(roc_auc_score(y_test,y_test_prob,labels = label_list,multi_class='ovr',average=\"weighted\"))\n",
    "    print(\"f1\")\n",
    "    print(metrics.f1_score(y_test,y_pred,labels=label_list,average=\"weighted\"))\n",
    "    print(\"acc\")\n",
    "    print(metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Run Simple Log Reg Model and Print metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "from sklearn_genetic import GASearchCV\n",
    "from sklearn_genetic.space import Continuous, Categorical, Integer\n",
    "import ast\n",
    "# Run log reg 10 times and average the result to reduce predction variance\n",
    "def run_log_reg(train_features, test_features, y_train, y_test, lbl_to_idx, idx_to_lbl,label_list,feature_name, apply=True):\n",
    "    y_train_idx = [lbl_to_idx[label] for label in y_train]\n",
    "    y_test_idx = [lbl_to_idx[label] for label in y_test]\n",
    "    label_idx = [i for i in range(len(label_list))]\n",
    "    models = [\n",
    "       MultinomialNB(),\n",
    "       SVC(probability=True),\n",
    "       RandomForestClassifier(),\n",
    "#       XGBClassifier(),\n",
    "       \n",
    "    ]\n",
    "    model_names=[\n",
    "        \"Naive bayes\",\n",
    "        \"SVM\",\n",
    "        \"RF\",\n",
    "#        \"XGBoost\"\n",
    "\n",
    "    ]\n",
    "    model_prams=[\n",
    "        {'alpha' : Continuous(0.01,0.5,distribution=\"log-uniform\")},\n",
    "        {\"kernel\" : Categorical(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "         \"C\" : Continuous(0.1,50,distribution=\"uniform\")\n",
    "        },\n",
    "        {\n",
    "            \"n_estimators\" : Integer(10,100),\n",
    "            \"max_depth\" : Integer(5,50),\n",
    "            \"min_samples_split\" : Integer(2,11),\n",
    "            \"min_samples_leaf\" : Integer(1,11),\n",
    "            \"criterion\" : Categorical([\"gini\", \"entropy\", \"log_loss\"]),\n",
    "            \"max_features\" : Integer(1,13)\n",
    "        }\n",
    "    ]\n",
    "    non_negs = [\"BOW\", \"TF-IDF\"]\n",
    "    if apply == True:\n",
    "        print(feature_name)\n",
    "        computed_params = pd.read_csv(f\"../result/all_params/{feature_name}.csv\", encoding='utf-8')\n",
    "        computed_params = computed_params.drop(columns=[\"Unnamed: 0\"])\n",
    "        compute_dict = computed_params.to_dict('dict')\n",
    "\n",
    "        result_dict ={}\n",
    "        for model,name in zip(models,model_names):\n",
    "            if feature_name not in non_negs and name ==\"Naive bayes\":\n",
    "                continue\n",
    "            clf = model\n",
    "            if name in compute_dict:\n",
    "                clf = model.set_params(**ast.literal_eval(compute_dict[name][0]))\n",
    "            clf.fit(train_features,y_train_idx)\n",
    "            y_test_prob = clf.predict_proba(test_features)\n",
    "            y_pred = clf.predict(test_features)\n",
    "            print_model_metrics(y_test_idx, y_test_prob, y_pred, label_idx,name)\n",
    "\n",
    "\n",
    "\n",
    "    pre_comp_best =[]\n",
    "    \n",
    "    random.seed(1)\n",
    "    cv = cv = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n",
    "    if apply:\n",
    "        return\n",
    "    for model, model_name, model_param in zip(models, model_names, model_prams):\n",
    "        try:\n",
    "            evolved_estimator = GASearchCV(estimator=model,\n",
    "                               cv=cv,\n",
    "                               scoring='accuracy',\n",
    "                               population_size=10,\n",
    "                               generations=35,\n",
    "                               param_grid=model_param,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=True,\n",
    "                               keep_top_k=4)        \n",
    "            evolved_estimator.fit(train_features,y_train_idx)\n",
    "            print(evolved_estimator.best_params_)\n",
    "            pre_comp_best.append({model_name :evolved_estimator.best_params_})\n",
    "        except ValueError:\n",
    "            print(\"hi\")\n",
    "    print(pre_comp_best)\n",
    "    #y_test_prob = model.predict_proba(test_features)\n",
    "    #y_pred = model.predict(test_features)\n",
    "    #print_model_metrics(y_test_idx,y_test_prob,y_pred,label_idx)\n",
    "    df = pd.DataFrame(pre_comp_best)\n",
    "    df.to_csv(f\"../result/all_search/{feature_name}.csv\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Bag-of-Words, TF-IDF and Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 2: 1, 5: 2, 6: 3, 8: 4, 11: 5, 13: 6, 14: 7, 15: 8, 16: 9, 18: 10, 19: 11, 21: 12, 27: 13, 28: 14, 32: 15, 33: 16, 39: 17, 42: 18, 44: 19, 46: 20, 50: 21, 53: 22}\n",
      "{0: 0, 1: 2, 2: 5, 3: 6, 4: 8, 5: 11, 6: 13, 7: 14, 8: 15, 9: 16, 10: 18, 11: 19, 12: 21, 13: 27, 14: 28, 15: 32, 16: 33, 17: 39, 18: 42, 19: 44, 20: 46, 21: 50, 22: 53}\n",
      "(325,)\n",
      "(169,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list= sorted(list(set(test.label.values)))\n",
    "lbl_to_idx = {item:i for i,item in enumerate(label_list)}\n",
    "idx_to_lbl = {i:item for i,item in enumerate(label_list)}\n",
    "print(lbl_to_idx)\n",
    "print(idx_to_lbl)\n",
    "y_train = train.label\n",
    "y_test = test.label\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "len(set(y_train.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Bag of Words\n",
    "Let's start with simple Bag-Of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW\n",
      "*****\n",
      "Naive bayes\n",
      "roc\n",
      "0.9971063268788243\n",
      "f1\n",
      "0.8832843612233645\n",
      "acc\n",
      "0.893491124260355\n",
      "*****\n",
      "SVM\n",
      "roc\n",
      "0.9923524725930808\n",
      "f1\n",
      "0.8833631041211363\n",
      "acc\n",
      "0.893491124260355\n",
      "*****\n",
      "RF\n",
      "roc\n",
      "0.9940404841025581\n",
      "f1\n",
      "0.9017367689781484\n",
      "acc\n",
      "0.9112426035502958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow = CountVectorizer()\n",
    "x_train = bow.fit_transform(train.title.values)\n",
    "x_test = bow.transform(test.title.values)\n",
    "\n",
    "run_log_reg(x_train, x_test, y_train, y_test, lbl_to_idx=lbl_to_idx, idx_to_lbl=idx_to_lbl, label_list=label_list,feature_name=\"BOW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "TFIDF should perform better than BoW since it uses document frequencies to normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF\n",
      "*****\n",
      "Naive bayes\n",
      "roc\n",
      "0.9972862406171837\n",
      "f1\n",
      "0.8956672832927013\n",
      "acc\n",
      "0.9053254437869822\n",
      "*****\n",
      "SVM\n",
      "roc\n",
      "0.995572179110172\n",
      "f1\n",
      "0.939688255470134\n",
      "acc\n",
      "0.9408284023668639\n",
      "*****\n",
      "RF\n",
      "roc\n",
      "0.9949796107841604\n",
      "f1\n",
      "0.9188677514635836\n",
      "acc\n",
      "0.9289940828402367\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "x_train = tfidf.fit_transform(train.title.values)\n",
    "x_test = tfidf.transform(test.title.values)\n",
    "\n",
    "run_log_reg(x_train, x_test, y_train, y_test, lbl_to_idx=lbl_to_idx, idx_to_lbl=idx_to_lbl, label_list=label_list,feature_name=\"TF-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF(Normalize)\n",
    "\n",
    "TFIDF should perform better than BoW since it uses document frequencies to normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def things_to_unit(a):\n",
    "    \"if 0.5km kind of that appears, convert to unitLength etc\"\n",
    "    doc_units = pd.read_excel(\"./normalizer/units.xlsx\")\n",
    "    doc_dict = dict(zip(doc_units[\"from\"],doc_units[\"to\"])) \n",
    "    for from_ in doc_dict:\n",
    "        idx = np.where(\n",
    "                 np.char.count(a,from_)==1\n",
    "              )\n",
    "        a[idx] = doc_dict[from_] \n",
    "    return a\n",
    "\n",
    "class LemmaPlaceTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`','(',')']\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        val = []\n",
    "        for t in word_tokenize(doc):\n",
    "            if t.isdigit():\n",
    "                val.append(\"unitN\")\n",
    "            elif (t not in self.ignore_tokens):\n",
    "                val.append(\n",
    "                    self.wnl.lemmatize(t,get_wordnet_pos(t))\n",
    "                )\n",
    "        new_val = np.array(val)\n",
    "        new_val = np.apply_along_axis(things_to_unit, 0, new_val)\n",
    "        return new_val\n",
    "\n",
    "def preprocess(document):\n",
    "    'changes document to lower case and removes stopwords'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words & numbrs\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\") or not word.isdigit()]\n",
    "\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF\n",
      "*****\n",
      "Naive bayes\n",
      "roc\n",
      "0.9972862406171837\n",
      "f1\n",
      "0.8956672832927013\n",
      "acc\n",
      "0.9053254437869822\n",
      "*****\n",
      "SVM\n",
      "roc\n",
      "0.9953875199637162\n",
      "f1\n",
      "0.939688255470134\n",
      "acc\n",
      "0.9408284023668639\n",
      "*****\n",
      "RF\n",
      "roc\n",
      "0.9936560397428671\n",
      "f1\n",
      "0.8967750738964523\n",
      "acc\n",
      "0.9053254437869822\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "x_train = tfidf.fit_transform(train.title.values)\n",
    "x_test = tfidf.transform(test.title.values)\n",
    "\n",
    "run_log_reg(x_train, x_test, y_train, y_test, lbl_to_idx=lbl_to_idx, idx_to_lbl=idx_to_lbl, label_list=label_list,feature_name=\"TF-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "TFIDF performs marginally better than BoW. Although whats impressive here is the fact that we're getting an F1 score of 0.826 with just 50 datapoints. This is why Log Reg + TFIDF is a great baseline for NLP classification tasks.\n",
    "\n",
    "Next we'll try 100D glove vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load the glove vectors with PyMagnitude\n",
    "# PyMagnitude is a fantastic library that handles a lot of word vectorization tasks. \n",
    "\n",
    "from pymagnitude import *\n",
    "from collections.abc import MutableMapping\n",
    "glove = Magnitude(\"../vectors/glove.6B.100d.magnitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/wr0_38q17_38rmx4dysxdyfh0000gn/T/ipykernel_4797/3747255845.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for title in tqdm_notebook(df.title.values):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4251ee6deb8b433e918231771aa607ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636d687262314e3c97e26ea3ce108985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We'll use Average Glove here \n",
    "from tqdm import tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "def avg_glove(df):\n",
    "    vectors = []\n",
    "    for title in tqdm_notebook(df.title.values):\n",
    "        vectors.append(np.average(glove.query(word_tokenize(title)), axis = 0))\n",
    "    return np.array(vectors)\n",
    "\n",
    "x_train = avg_glove(train)\n",
    "x_test = avg_glove(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOVE\n",
      "*****\n",
      "SVM\n",
      "roc\n",
      "0.9928964371959328\n",
      "f1\n",
      "0.9164027147828975\n",
      "acc\n",
      "0.9230769230769231\n",
      "*****\n",
      "RF\n",
      "roc\n",
      "0.9937596052758378\n",
      "f1\n",
      "0.8621848732097491\n",
      "acc\n",
      "0.8757396449704142\n"
     ]
    }
   ],
   "source": [
    "run_log_reg(x_train, x_test, y_train, y_test, lbl_to_idx=lbl_to_idx, idx_to_lbl=idx_to_lbl, label_list=label_list,feature_name=\"GLOVE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT\n",
      "*****\n",
      "SVM\n",
      "roc\n",
      "0.9845586385619066\n",
      "f1\n",
      "0.83778963863104\n",
      "acc\n",
      "0.8520710059171598\n",
      "*****\n",
      "RF\n",
      "roc\n",
      "0.9468554476720126\n",
      "f1\n",
      "0.6604970554635398\n",
      "acc\n",
      "0.6745562130177515\n"
     ]
    }
   ],
   "source": [
    "x_train = np.genfromtxt(\"../datasets/train_feature_bert-base-uncased.csv\",delimiter=\",\")\n",
    "x_test = np.genfromtxt(\"../datasets/test_feature_bert-base-uncased.csv\",delimiter=\",\")\n",
    "y_train = np.genfromtxt(\"../datasets/train_label_bert-base-uncased.csv\",delimiter=\",\")\n",
    "y_test = np.genfromtxt(\"../datasets/test_label_bert-base-uncased.csv\",delimiter=\",\")\n",
    "\n",
    "run_log_reg(x_train, x_test, y_train, y_test, lbl_to_idx=lbl_to_idx, idx_to_lbl=idx_to_lbl, label_list=label_list,feature_name=\"BERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SenBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENBERT\n",
      "*****\n",
      "SVM\n",
      "roc\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of given labels, 23, not equal to the number of columns in 'y_score', 19",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mgenfromtxt(\u001b[39m\"\u001b[39m\u001b[39m../datasets/train_label_sbert.csv\u001b[39m\u001b[39m\"\u001b[39m,delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m y_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mgenfromtxt(\u001b[39m\"\u001b[39m\u001b[39m../datasets/test_label_sbert.csv\u001b[39m\u001b[39m\"\u001b[39m,delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m run_log_reg(x_train, x_test, y_train, y_test, lbl_to_idx\u001b[39m=\u001b[39;49mlbl_to_idx, idx_to_lbl\u001b[39m=\u001b[39;49midx_to_lbl, label_list\u001b[39m=\u001b[39;49mlabel_list,feature_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSENBERT\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [20], line 63\u001b[0m, in \u001b[0;36mrun_log_reg\u001b[0;34m(train_features, test_features, y_train, y_test, lbl_to_idx, idx_to_lbl, label_list, feature_name, apply)\u001b[0m\n\u001b[1;32m     61\u001b[0m         y_test_prob \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict_proba(test_features)\n\u001b[1;32m     62\u001b[0m         y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(test_features)\n\u001b[0;32m---> 63\u001b[0m         print_model_metrics(y_test_idx, y_test_prob, y_pred, label_idx,name)\n\u001b[1;32m     67\u001b[0m pre_comp_best \u001b[39m=\u001b[39m[]\n\u001b[1;32m     69\u001b[0m random\u001b[39m.\u001b[39mseed(\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn [19], line 22\u001b[0m, in \u001b[0;36mprint_model_metrics\u001b[0;34m(y_test, y_test_prob, y_pred, label_list, name)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(name)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mroc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[39mprint\u001b[39m(roc_auc_score(y_test,y_test_prob,labels \u001b[39m=\u001b[39;49m label_list,multi_class\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39movr\u001b[39;49m\u001b[39m'\u001b[39;49m,average\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mweighted\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(metrics\u001b[39m.\u001b[39mf1_score(y_test,y_pred,labels\u001b[39m=\u001b[39mlabel_list,average\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/smallData/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:565\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[39mif\u001b[39;00m multi_class \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    564\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmulti_class must be in (\u001b[39m\u001b[39m'\u001b[39m\u001b[39movo\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39movr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m _multiclass_roc_auc_score(\n\u001b[1;32m    566\u001b[0m         y_true, y_score, labels, multi_class, average, sample_weight\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[39melif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    569\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y_true)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/smallData/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:665\u001b[0m, in \u001b[0;36m_multiclass_roc_auc_score\u001b[0;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be ordered\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    664\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(classes) \u001b[39m!=\u001b[39m y_score\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 665\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    666\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNumber of given labels, \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m, not equal to the number \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    667\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mof columns in \u001b[39m\u001b[39m'\u001b[39m\u001b[39my_score\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(classes), y_score\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[1;32m    668\u001b[0m     )\n\u001b[1;32m    669\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39msetdiff1d(y_true, classes)):\n\u001b[1;32m    670\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m'\u001b[39m\u001b[39m contains labels not in parameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Number of given labels, 23, not equal to the number of columns in 'y_score', 19"
     ]
    }
   ],
   "source": [
    "x_train = np.genfromtxt(\"../datasets/train_feature_sbert.csv\",delimiter=\",\")\n",
    "x_test = np.genfromtxt(\"../datasets/test_feature_sbert.csv\",delimiter=\",\")\n",
    "y_train = np.genfromtxt(\"../datasets/train_label_sbert.csv\",delimiter=\",\")\n",
    "y_test = np.genfromtxt(\"../datasets/test_label_sbert.csv\",delimiter=\",\")\n",
    "\n",
    "run_log_reg(x_train, x_test, y_train, y_test, lbl_to_idx=lbl_to_idx, idx_to_lbl=idx_to_lbl, label_list=label_list,feature_name=\"SENBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electra\n",
      "*****\n",
      "SVM\n",
      "roc\n",
      "0.9757177256856888\n",
      "f1\n",
      "0.7503411617733708\n",
      "acc\n",
      "0.757396449704142\n",
      "*****\n",
      "RF\n",
      "roc\n",
      "0.9394809369268763\n",
      "f1\n",
      "0.6144681267255859\n",
      "acc\n",
      "0.6390532544378699\n"
     ]
    }
   ],
   "source": [
    "x_train = np.genfromtxt(\"../datasets/train_feature_electra-small-discriminator.csv\",delimiter=\",\")\n",
    "x_test = np.genfromtxt(\"../datasets/test_feature_electra-small-discriminator.csv\",delimiter=\",\")\n",
    "y_train = np.genfromtxt(\"../datasets/train_label_electra-small-discriminator.csv\",delimiter=\",\")\n",
    "y_test = np.genfromtxt(\"../datasets/test_label_electra-small-discriminator.csv\",delimiter=\",\")\n",
    "\n",
    "run_log_reg(x_train, x_test, y_train, y_test, lbl_to_idx=lbl_to_idx, idx_to_lbl=idx_to_lbl, label_list=label_list,feature_name=\"Electra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNET\n",
      "*****\n",
      "SVM\n",
      "roc\n",
      "0.9860460143468529\n",
      "f1\n",
      "0.8038650237836859\n",
      "acc\n",
      "0.8047337278106509\n",
      "*****\n",
      "RF\n",
      "roc\n",
      "0.9759992815352431\n",
      "f1\n",
      "0.6972566486068941\n",
      "acc\n",
      "0.7218934911242604\n"
     ]
    }
   ],
   "source": [
    "x_train = np.genfromtxt(\"../datasets/train_feature_fnet-base.csv\",delimiter=\",\")\n",
    "x_test = np.genfromtxt(\"../datasets/test_feature_fnet-base.csv\",delimiter=\",\")\n",
    "y_train = np.genfromtxt(\"../datasets/train_label_fnet-base.csv\",delimiter=\",\")\n",
    "y_test = np.genfromtxt(\"../datasets/test_label_fnet-base.csv\",delimiter=\",\")\n",
    "\n",
    "run_log_reg(x_train, x_test, y_train, y_test, lbl_to_idx=lbl_to_idx, idx_to_lbl=idx_to_lbl, label_list=label_list, feature_name=\"FNET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(325, 768)\n",
      "FNET\n",
      "*****\n",
      "SVM\n",
      "roc\n",
      "0.958329303790772\n",
      "f1\n",
      "0.7032297213170865\n",
      "acc\n",
      "0.7100591715976331\n",
      "*****\n",
      "RF\n",
      "roc\n",
      "0.8775023449144704\n",
      "f1\n",
      "0.525083967687518\n",
      "acc\n",
      "0.5443786982248521\n"
     ]
    }
   ],
   "source": [
    "x_train = np.genfromtxt(\"../datasets/train_feature_roberta-base.csv\",delimiter=\",\")\n",
    "x_test = np.genfromtxt(\"../datasets/test_feature_roberta-base.csv\",delimiter=\",\")\n",
    "y_train = np.genfromtxt(\"../datasets/train_label_roberta-base.csv\",delimiter=\",\")\n",
    "y_test = np.genfromtxt(\"../datasets/test_label_roberta-base.csv\",delimiter=\",\")\n",
    "run_log_reg(x_train, x_test, y_train, y_test, lbl_to_idx=lbl_to_idx, idx_to_lbl=idx_to_lbl, label_list=label_list, feature_name=\"FNET\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('smallData')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "204ba388238a6e55b9feb9487e3d718d3b3259521e8492c709b3886a91f63210"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
